{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 1.2.0\n",
      "GPU-enabled installation? False\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version {}\".format(torch.__version__))\n",
    "print(\"GPU-enabled installation? {}\".format(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calling the constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00, -2.0000e+00,  0.0000e+00],\n",
      "        [-2.0000e+00,  7.3787e+22,  2.4176e-12]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor(2, 3)\n",
    "print(t)\n",
    "print(t.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor([[1, 2, 3], [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calling a method in the torch module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 64-bit integer tensor: tensor([1, 2, 3]), torch.LongTensor\n",
      "A 32-bit float tensor: tensor([1., 2., 3.]), torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "tl = torch.tensor([1, 2, 3])\n",
    "t = torch.tensor([1., 2., 3.])\n",
    "print(\"A 64-bit integer tensor: {}, {}\".format(tl, tl.type()))\n",
    "print(\"A 32-bit float tensor: {}, {}\".format(t, t.type()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.zeros(2, 3)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]])\n",
      "tensor([[0.1837, 0.1376, 0.7396],\n",
      "        [0.1249, 0.3281, 0.1789]])\n",
      "tensor([[-0.5177, -0.7123, -0.4389],\n",
      "        [ 0.1121, -1.3473, -1.0835]])\n"
     ]
    }
   ],
   "source": [
    "t_zeros = torch.zeros_like(t)        # zeros_like returns a new tensor\n",
    "t_ones = torch.ones(2, 3)            # creates a tensor with 1s\n",
    "t_fives = torch.empty(2, 3).fill_(5) # creates a non-initialized tensor and fills it with 5\n",
    "t_random = torch.rand(2, 3)          # creates a uniform random tensor\n",
    "t_normal = torch.randn(2, 3)         # creates a normal random tensor\n",
    "\n",
    "print(t_zeros)\n",
    "print(t_ones)\n",
    "print(t_fives)\n",
    "print(t_random)\n",
    "print(t_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the computational graph (see below)\n",
    "t1 = torch.clone(t)\n",
    "assert id(t) != id(t1), 'Functional methods create a new copy of the tensor'\n",
    "\n",
    "# To create a new _independent_ copy, we do need to detach \n",
    "# from the graph\n",
    "t1 = torch.clone(t).detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the PyTorch–NumPy bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy array: [1. 2. 3.], type: float64\n",
      "Torch tensor: tensor([1., 2., 3.], dtype=torch.float64), type: torch.float64\n"
     ]
    }
   ],
   "source": [
    "# Create a new multi-dimensional array in NumPy with the np datatype (np.float32)\n",
    "a = np.array([1., 2., 3.])\n",
    "\n",
    "# Convert the array to a torch tensor\n",
    "t = torch.tensor(a)\n",
    "\n",
    "print(\"NumPy array: {}, type: {}\".format(a, a.dtype))\n",
    "print(\"Torch tensor: {}, type: {}\".format(t, t.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 3.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6231, 0.5008])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn(2, 3)\n",
    "t[ : , 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0279, -0.3199, -0.6748,  0.1461,  0.9728,  1.0245],\n",
      "        [-1.3091, -0.6430,  0.0109, -1.2471,  0.6354, -0.1463],\n",
      "        [-1.7598, -2.4160,  0.5159, -1.2517,  0.1154,  0.8764],\n",
      "        [ 0.9015,  1.5617, -0.6592,  1.2284, -0.3083, -0.1000],\n",
      "        [-0.2604, -0.6674, -1.4130, -1.4552, -0.1316,  0.2240]])\n",
      "tensor([[-1.3091, -0.6430,  0.0109, -1.2471,  0.6354, -0.1463],\n",
      "        [ 0.9015,  1.5617, -0.6592,  1.2284, -0.3083, -0.1000]])\n",
      "tensor([ 0.6354, -0.1000])\n"
     ]
    }
   ],
   "source": [
    "t = torch.randn(5, 6)\n",
    "print(t)\n",
    "i = torch.tensor([1, 3])\n",
    "j = torch.tensor([4, 5])\n",
    "print(t[i])                          # selects rows 1 and 3\n",
    "print(t[i, j])                       # selects (1, 4) and (3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0279, -0.3199, -0.6748,  0.1461,  0.9728,  1.0245],\n",
      "        [-1.3091, -0.6430,  0.0109, -1.2471,  0.6354, -0.1463],\n",
      "        [-1.7598, -2.4160,  0.5159, -1.2517,  0.1154,  0.8764],\n",
      "        [ 0.9015,  1.5617, -0.6592,  1.2284, -0.3083, -0.1000],\n",
      "        [-0.2604, -0.6674, -1.4130, -1.4552, -0.1316,  0.2240]])\n",
      "tensor([[-0.0279, -0.3199, -0.6748,  0.1461,  0.9728,  1.0245],\n",
      "        [-1.3091, -0.6430,  0.0109, -1.2471,  0.6354, -0.1463],\n",
      "        [-1.7598, -2.4160,  0.5159, -1.2517,  0.1154,  0.8764],\n",
      "        [ 0.9015,  1.5617, -0.6592,  1.2284, -0.3083, -0.1000],\n",
      "        [-0.2604, -0.6674, -1.4130, -1.4552, -0.1316,  0.2240]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[  0,   0,   0,   0,   0,   1],\n",
      "        [255,   0,   0, 255,   0,   0],\n",
      "        [255, 254,   0, 255,   0,   0],\n",
      "        [  0,   1,   0,   1,   0,   0],\n",
      "        [  0,   0, 255, 255,   0,   0]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "t = t.float()   # converts to 32-bit float\n",
    "print(t)\n",
    "t = t.double()  # converts to 64-bit float\n",
    "print(t)\n",
    "t = t.byte()    # converts to unsigned 8-bit integer\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations on tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(42)\n"
     ]
    }
   ],
   "source": [
    "# Scalars =: creates a tensor with a scalar \n",
    "# (zero-th order tensor,  i.e. just a number)\n",
    "s = torch.tensor(42)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row vector\n",
      "tensor([[-1.5524,  0.8494,  0.5094]])\n",
      "with size torch.Size([1, 3]) \n",
      "\n",
      "Column vector\n",
      "tensor([[-0.9034],\n",
      "        [ 0.8244],\n",
      "        [ 0.1981]])\n",
      "with size torch.Size([3, 1]) \n",
      "\n",
      "Matrix\n",
      "tensor([[-0.7321, -0.8348, -0.5578],\n",
      "        [-0.4026, -0.7836, -0.3000],\n",
      "        [-2.0847,  1.2997,  0.5293]])\n",
      "with size torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Row vector\n",
    "x = torch.randn(1,3)\n",
    "print(\"Row vector\\n{}\\nwith size {}\".format(x, x.size()), '\\n')\n",
    "\n",
    "# Column vector\n",
    "v = torch.randn(3,1)\n",
    "print(\"Column vector\\n{}\\nwith size {}\".format(v, v.size()), '\\n')\n",
    "\n",
    "# Matrix\n",
    "A = torch.randn(3, 3)\n",
    "print(\"Matrix\\n{}\\nwith size {}\".format(A, A.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1373],\n",
      "        [-0.3417],\n",
      "        [ 3.0597]]) \n",
      "\n",
      "tensor([[-1.5258],\n",
      "        [ 0.1881],\n",
      "        [ 4.1542]])\n"
     ]
    }
   ],
   "source": [
    "u = torch.matmul(A, v)\n",
    "print(u, '\\n')\n",
    "b = torch.randn(3,1)\n",
    "y = u + b              # we can also do torch.add(u, b)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.481540560722351\n"
     ]
    }
   ],
   "source": [
    "s = torch.matmul(x, torch.matmul(A, v))\n",
    "print(s.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7497,  0.5296,  1.1363],\n",
       "        [ 0.2596,  0.2451, -1.7299]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# common tensor methods (they also have the counterpart in the torch package, e.g. as torch.sum(t))\n",
    "t = torch.randn(2,3)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4901,  0.7747, -0.5935])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7497,  0.2596],\n",
       "        [ 0.5296,  0.2451],\n",
       "        [ 1.1363, -1.7299]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.t()                   # transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.numel()               # number of elements in tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [0, 1],\n",
       "        [0, 2],\n",
       "        [1, 0],\n",
       "        [1, 1],\n",
       "        [1, 2]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.nonzero()             # indices of non-zero elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7497,  0.5296],\n",
       "        [ 1.1363,  0.2596],\n",
       "        [ 0.2451, -1.7299]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.view(-1, 2)           # reorganizes the tensor to these dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7497,  0.5296,  1.1363],\n",
       "        [ 0.2596,  0.2451, -1.7299]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.squeeze()             # removes size 1 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7497,  0.5296,  1.1363],\n",
       "         [ 0.2596,  0.2451, -1.7299]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.unsqueeze(0)          # inserts a dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# operations in the package\n",
    "torch.arange(0, 10)     # tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(3, 3)         # creates a 3x3 matrix with 1s in the diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(0, 3)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 0, 1, 2])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((t, t))       # tensor([0, 1, 2, 0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [0, 1, 2]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack((t, t))     # tensor([[0, 1, 2],\n",
    "                        #         [0, 1, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_gpu = torch.cuda.FloatTensor(3, 3)   # creation of a GPU tensor\n",
    "t_gpu.zero_()                          # initialization to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch not compiled with CUDA enabled\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    t_gpu = torch.randn(3, 3, device=\"cuda:0\")\n",
    "except:\n",
    "    print(\"Torch not compiled with CUDA enabled\")\n",
    "    t_gpu = None\n",
    "    \n",
    "t_gpu  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1597, -1.2119, -0.7052],\n",
       "        [-0.0348,  1.0689, -0.5973],\n",
       "        [-0.4529, -0.4263,  0.0266]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we could also state explicitly the device to be the \n",
    "# CPU with torch.randn(3,3,device=\"cpu\")\n",
    "t = torch.randn(3, 3)   \n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_gpu = t.to(\"cuda:0\")  # copies the tensor from CPU to GPU\n",
    "# note that if we do now t_to_gpu.to(\"cuda:0\") it will \n",
    "# return the same tensor without doing anything else \n",
    "# as this tensor already resides on the GPU\n",
    "print(t_gpu)\n",
    "print(t_gpu.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1597, -1.2119, -0.7052],\n",
       "        [-0.0348,  1.0689, -0.5973],\n",
       "        [-0.4529, -0.4263,  0.0266]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# moves t to the device (this code will **not** fail if the \n",
    "# local machine has not access to a GPU)\n",
    "t.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network foundations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Static graphs:** the deep learning framework converts the computational graph into a static representation that cannot be modified. This allows the library developers to do very aggressive optimizations on this static graph ahead of computation time, pruning some areas and transforming others so that the final product is highly optimized and fast. The drawback is that some models can be really hard to implement with this approach. For example, TensorFlow uses static graphs. Having static graphs is part of the reason why TensorFlow has excellent support for sequence processing, which makes it very popular in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dynamic graphs:** the framework does not create a graph ahead of computation, but records the operations that are performed, which can be quite different for different inputs. When it is time to compute the gradients, it unrolls the graph and perform the computations. A major benefit of this approach is that implementing complex models can be easier in this paradigm. This flexibility comes at the expense of the major drawback of this approach: speed. Dynamic graphs cannot leverage the same level of ahead-of-time optimization as static graphs, which makes them slower. PyTorch uses dynamic graphs as the underlying paradigm for gradient computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyCustomModule(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden, n_output_classes):\n",
    "        # call super to initialize the class above in the hierarchy\n",
    "        super(MyCustomModule, self).__init__()\n",
    "        # first affine transformation\n",
    "        self.W = nn.Linear(n_inputs, n_hidden)        \n",
    "        # non-linearity (here it is also a layer!)\n",
    "        self.f = nn.ReLU()\n",
    "        # final affine transformation\n",
    "        self.U = nn.Linear(n_hidden, n_output_classes) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.U(self.f(self.W(x)))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1543, -0.1493]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# set the network's architectural parameters\n",
    "n_inputs = 3\n",
    "n_hidden= 4\n",
    "n_output_classes = 2\n",
    "\n",
    "# instantiate the model\n",
    "model = MyCustomModule(n_inputs, n_hidden, n_output_classes)\n",
    "\n",
    "# create a simple input tensor \n",
    "# size is [1,3]: a mini-batch of one example, \n",
    "# this example having dimension 3\n",
    "x = torch.FloatTensor([[0.3, 0.8, -0.4]]) \n",
    "\n",
    "# compute the model output by **applying** the input to the module\n",
    "y = model(x)\n",
    "\n",
    "# inspect the output\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyCustomModule(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden, n_output_classes):\n",
    "        super(MyCustomModule, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(n_inputs, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_output_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.network(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyCustomModule(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden, n_output_classes):\n",
    "        super(MyCustomModule, self).__init__()\n",
    "        self.p_keep = 0.7\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(n_inputs, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, 2*n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2*n_hidden, n_output_classes),   \n",
    "            # dropout argument is probability of dropping\n",
    "            nn.Dropout(1 - self.p_keep),\n",
    "            # applies softmax in the data dimension\n",
    "            nn.Softmax(dim=1)                  \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.network(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 5.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "y = F.relu(torch.FloatTensor([[-5, -1, 0, 5]]))\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8564, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# the true label (in this case, 2) from our dataset wrapped \n",
    "# as a tensor of minibatch size of 1\n",
    "y_gold = torch.tensor([1])        \n",
    "                                  \n",
    "# our simple classification criterion for this simple example    \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "# forward pass of our model (remember, using apply instead of forward)\n",
    "y = model(x)  \n",
    "\n",
    "# apply the criterion to get the loss corresponding to the pair (x, y)\n",
    "# with respect to the real y (y_gold)\n",
    "loss = criterion(y, y_gold)       \n",
    "                                 \n",
    "\n",
    "# the loss contains a gradient function that we can use to compute\n",
    "# the gradient dL/dw (gradient with respect to the parameters \n",
    "# for a given fixed input)\n",
    "print(loss)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First-order dynamics: \n",
    "- Search direction only: optim.SGD\n",
    "- Adaptive: optim.RMSprop, optim.Adagrad, optim.Adadelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second-order dynamics\n",
    "- Search direction only: Momentum optim.SGD(momentum=0.9), Nesterov, optim.SGD(nesterov=True)\n",
    "- Adaptive: optim.Adam, optim.Adamax (Adam with L∞)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a simple mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M = 1200\n",
    "\n",
    "# sample from the x axis M points\n",
    "x = np.random.rand(M) * 2*math.pi\n",
    "\n",
    "# add noise\n",
    "eta = np.random.rand(M) * 0.01\n",
    "\n",
    "# compute the function\n",
    "y = np.sin(x) + eta\n",
    "\n",
    "# plot\n",
    "_ = plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the NumPy-PyTorch bridge\n",
    "x_train = torch.tensor(x[0:1000]).float().view(-1, 1).to(device)\n",
    "y_train = torch.tensor(y[0:1000]).float().view(-1, 1).to(device)\n",
    "\n",
    "x_test = torch.tensor(x[1000:]).float().view(-1, 1).to(device)\n",
    "y_test = torch.tensor(y[1000:]).float().view(-1, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SineDataset(data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super(SineDataset, self).__init__()\n",
    "        assert x.shape[0] == y.shape[0]\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "sine_dataset = SineDataset(x_train, y_train)\n",
    "\n",
    "sine_dataset_test = SineDataset(x_test, y_test)\n",
    "\n",
    "sine_loader = torch.utils.data.DataLoader(\n",
    "    sine_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "sine_loader_test = torch.utils.data.DataLoader(\n",
    "    sine_dataset_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SineModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SineModel, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(1, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# declare the model\n",
    "model = SineModel().to(device)\n",
    "\n",
    "# define the criterion\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# select the optimizer and pass to it the parameters of the model it will optimize\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "    for i, (x_i, y_i) in enumerate(sine_loader):\n",
    "\n",
    "        y_hat_i = model(x_i)            # forward pass\n",
    "                                \n",
    "        loss = criterion(y_hat_i, y_i)  # compute the loss and perform the backward pass\n",
    "\n",
    "        optimizer.zero_grad()           # cleans the gradients\n",
    "        loss.backward()                 # computes the gradients\n",
    "        optimizer.step()                # update the parameters\n",
    "\n",
    "    if epoch % 20:\n",
    "        plt.scatter(x_i.data.cpu().numpy(), y_hat_i.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0038324657070916146\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    for k, (x_k, y_k) in enumerate(sine_loader_test):\n",
    "        y_hat_k = model(x_k)\n",
    "        loss_test = criterion(y_hat_k, y_k)\n",
    "        total_loss += float(loss_test)\n",
    "\n",
    "print(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def enforce_reproducibility(seed=42):\n",
    "    # Sets seed manually for both CPU and CUDA\n",
    "    torch.manual_seed(seed)\n",
    "    # For atomic operations there is currently \n",
    "    # no simple way to enforce determinism, as\n",
    "    # the order of parallel operations is not known.\n",
    "    #\n",
    "    # CUDNN\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # System based\n",
    "    np.random.seed(seed)\n",
    "\n",
    "enforce_reproducibility()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
